{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CnnClassifier(nn.Module):\n",
    "    # n_hidden: number of units at the last fc layer\n",
    "    def __init__(self, n_hidden, pt_layers=None):\n",
    "        super(CnnClassifier, self).__init__()\n",
    "\n",
    "        # in_data size: (batch_size, 1, 28, 28)\n",
    "        self.cnn_layers = nn.Sequential(\n",
    "            # conv1_out size: (batch_size, 4, 26, 26)\n",
    "            nn.Conv2d(in_channels=1, out_channels=4, kernel_size=(3, 3), stride=(1, 1), padding=0),\n",
    "            nn.ReLU(),\n",
    "            # conv2_out size: (batch_size, 8, 12, 12)\n",
    "            nn.Conv2d(in_channels=4, out_channels=8, kernel_size=(3, 3), stride=(2, 2), padding=0),\n",
    "            nn.ReLU(),\n",
    "            # conv3_out size: (batch_size, 16, 5, 5)\n",
    "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3, 3), stride=(2, 2), padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=0, dilation=1, return_indices=False, ceil_mode=False),\n",
    "            # conv4_out size: (batch_size, 32, 1, 1)\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 3), stride=(1, 1), padding=0),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        if pt_layers is not None:\n",
    "            self.cnn_layers.load_state_dict(pt_layers)\n",
    "        # linear layers transforms flattened image features into logits before the softmax layer\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(32, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, 47)  # there are 10 classes\n",
    "        )\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.loss_fn = nn.CrossEntropyLoss(reduction='sum')  # will be divided by batch size\n",
    "\n",
    "    def forward(self, in_data):\n",
    "        img_features = self.cnn_layers(in_data).view(in_data.size(0), 32)\n",
    "        logits = self.linear(img_features)\n",
    "        return logits\n",
    "\n",
    "    def loss(self, logits, labels):\n",
    "        return self.loss_fn(logits, labels) / logits.size(0)\n",
    "\n",
    "    def top1_accuracy(self, logits, labels):\n",
    "        # get argmax of logits along dim=1 (this is equivalent to argmax of predicted probabilites)\n",
    "        predicted_labels = torch.argmax(logits, dim=1, keepdim=False)  # size (batch_size,)\n",
    "        n_corrects = predicted_labels.eq(labels).sum(0)  # sum up all the correct predictions\n",
    "        return n_corrects / logits.size(0) * 100  # in percentage\n",
    "\n",
    "    def topk_accuracy(self, k, logits, labels):\n",
    "        predicted_labels = torch.topk(logits, k, dim=1)[1]\n",
    "        n_corrects = torch.tensor([0])\n",
    "        for i in range(k):\n",
    "            n_corrects += predicted_labels[:, i].eq(labels).sum(0)\n",
    "        return n_corrects.double() / logits.size(0) * 100  # in percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class LocalEMNIST(Dataset):\n",
    "    '''\n",
    "    ds: emnist dataset provided in project files\n",
    "    flatten: bool, flatten it or not\n",
    "    '''\n",
    "\n",
    "    def __init__(self, ds, flatten):\n",
    "        self.flatten = flatten\n",
    "        self.ds = ds\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image, label = self.ds[index]\n",
    "        image = image.view(-1) if self.flatten else image\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import EMNIST\n",
    "from torch.utils.data import ConcatDataset, Subset\n",
    "from torchvision.transforms import ToTensor, Compose\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def show_image(dataset, index):\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.imshow(dataset[index][0][0], cmap=plt.get_cmap('gray'))\n",
    "\n",
    "\n",
    "def get_datasets(split='balanced', save=False):\n",
    "    download_folder = './data'\n",
    "\n",
    "    transform = Compose([ToTensor()])\n",
    "\n",
    "    dataset = ConcatDataset([EMNIST(root=download_folder, split=split, download=True, train=False, transform=transform),\n",
    "                             EMNIST(root=download_folder, split=split, download=True, train=True, transform=transform)])\n",
    "\n",
    "    # Ignore the code below with argument 'save'\n",
    "    if save:\n",
    "        random_seed = 4211  # do not change\n",
    "        n_samples = len(dataset)\n",
    "        eval_size = 0.2\n",
    "        indices = list(range(n_samples))\n",
    "        split = int(np.floor(eval_size * n_samples))\n",
    "\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        train_indices, eval_indices = indices[split:], indices[:split]\n",
    "\n",
    "        # cut to half\n",
    "        train_indices = train_indices[:len(train_indices) // 2]\n",
    "        eval_indices = eval_indices[:len(eval_indices) // 2]\n",
    "\n",
    "        np.savez('train_test_split.npz', train=train_indices, test=eval_indices)\n",
    "\n",
    "    # just use save=False for students\n",
    "    # load train test split indices\n",
    "    else:\n",
    "        with np.load('./train_test_split.npz') as f:\n",
    "            train_indices = f['train']\n",
    "            eval_indices = f['test']\n",
    "\n",
    "    train_dataset = Subset(dataset, indices=train_indices)\n",
    "    eval_dataset = Subset(dataset, indices=eval_indices)\n",
    "\n",
    "    return train_dataset, eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "import torch\n",
    "\n",
    "\n",
    "def train(model, loaders, optimizer, writer, n_epochs, ckpt_path, device='cpu', train_type=''):\n",
    "    def run_epoch(train_or_eval):\n",
    "        epoch_loss = 0.\n",
    "        epoch_acc = 0.\n",
    "        epoch_acc_top3 = 0.\n",
    "        for i, batch in enumerate(loaders[train_or_eval], 1):\n",
    "            in_data, labels = batch\n",
    "            in_data, labels = in_data.to(device), labels.to(device)\n",
    "            \n",
    "            if train_or_eval == 'train':\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            logits = model(in_data)\n",
    "            batch_loss = model.loss(logits, labels)\n",
    "            batch_acc = model.topk_accuracy(1, logits, labels)\n",
    "            batch_acc_top3 = model.topk_accuracy(3, logits, labels)\n",
    "\n",
    "            epoch_loss += batch_loss.item()\n",
    "            epoch_acc += batch_acc.item()\n",
    "            epoch_acc_top3 += batch_acc_top3.item()\n",
    "\n",
    "            if train_or_eval == 'train':\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        epoch_loss /= i\n",
    "        epoch_acc /= i\n",
    "        epoch_acc_top3 /= i\n",
    "\n",
    "        losses[train_or_eval] = epoch_loss\n",
    "        accs_top1[train_or_eval] = epoch_acc\n",
    "        accs_top3[train_or_eval] = epoch_acc_top3\n",
    "\n",
    "        if writer is None:\n",
    "            print('epoch %d %s loss %.4f acc %.4f acc_top3 %.4f' % (epoch, train_or_eval, epoch_loss, epoch_acc, epoch_acc_top3))\n",
    "        elif train_or_eval == 'eval':\n",
    "            name = \"%s-n_hid-%d-lr-%.3f\" % (train_type, n_hidden, optimizer.param_groups[0][\"lr\"])\n",
    "            writer.add_scalars('%s_loss' % name,\n",
    "                               tag_scalar_dict={'train': losses['train'],\n",
    "                                                'eval': losses['eval']},\n",
    "                               global_step=epoch)\n",
    "\n",
    "            writer.add_scalars('%s_top1_accuracy' % name,\n",
    "                               tag_scalar_dict={'train': accs_top1['train'],\n",
    "                                                'eval': accs_top1['eval']},\n",
    "                               global_step=epoch)\n",
    "\n",
    "            writer.add_scalars('%s_top3_accuracy' % name,\n",
    "                               tag_scalar_dict={'train': accs_top3['train'],\n",
    "                                                'eval': accs_top3['eval']},\n",
    "                               global_step=epoch)\n",
    "\n",
    "    # main statements\n",
    "    losses = dict()\n",
    "    accs_top1 = dict()\n",
    "    accs_top3 = dict()\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        run_epoch('train')\n",
    "        run_epoch('eval')\n",
    "\n",
    "        # For instructional purpose, show how to save checkpoints\n",
    "        if ckpt_path is not None:\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'losses': losses,\n",
    "                'accs': accs_top1\n",
    "            }, '%s/%d.pt' % (ckpt_path, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, eval_ds = get_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from torch.optim import Adam, SGD\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "ckpt_path = None\n",
    "n_epochs = 50\n",
    "batch_size = 128\n",
    "gpu = 0\n",
    "\n",
    "params = [{\"n_hidden\": 32, \"opt_class\": Adam, \"lr\": 0.001},\n",
    "          {\"n_hidden\": 32, \"opt_class\": SGD, \"lr\": 0.1},\n",
    "          {\"n_hidden\": 32, \"opt_class\": SGD, \"lr\": 0.01},\n",
    "          {\"n_hidden\": 64, \"opt_class\": Adam, \"lr\": 0.001},\n",
    "          {\"n_hidden\": 64, \"opt_class\": SGD, \"lr\": 0.1},\n",
    "          {\"n_hidden\": 64, \"opt_class\": SGD, \"lr\": 0.01},]\n",
    "\n",
    "if torch.cuda.is_available() and gpu != -1:\n",
    "    DEVICE = gpu\n",
    "else:\n",
    "    DEVICE = 'cpu'\n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(LocalEMNIST(train_ds, flatten=False), batch_size=batch_size, drop_last=False, shuffle=True),\n",
    "    'eval': DataLoader(LocalEMNIST(eval_ds, flatten=False), batch_size=batch_size, drop_last=False)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_eval_indices = np.random.choice(train_ds.indices, int(train_ds.indices.size/5), replace=False)\n",
    "validation_train_indices = np.setdiff1d(train_ds.indices, validation_eval_indices)\n",
    "\n",
    "validation_eval = Subset(train_ds.dataset, indices=validation_eval_indices)\n",
    "validation_train = Subset(train_ds.dataset, indices=validation_train_indices)\n",
    "\n",
    "dataloaders_validation = {\n",
    "    'train': DataLoader(LocalEMNIST(validation_train, flatten=False), batch_size=batch_size, drop_last=False, shuffle=True),\n",
    "    'eval': DataLoader(LocalEMNIST(validation_eval, flatten=False), batch_size=batch_size, drop_last=False)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in params:\n",
    "    n_hidden = param[\"n_hidden\"]\n",
    "    opt_class = param[\"opt_class\"]\n",
    "    lr = param[\"lr\"]\n",
    "    \n",
    "    print(\"Performing training for: \" + str(param))\n",
    "    \n",
    "    model = CnnClassifier(n_hidden).to(DEVICE)\n",
    "    optimizer = opt_class(model.parameters(), lr=lr)\n",
    "    writer = SummaryWriter('./logs/cnn/%s' % opt_class.__name__)\n",
    "    \n",
    "    train(model, dataloaders_validation, optimizer, writer, n_epochs, ckpt_path, DEVICE, \"scratch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing training for: {'n_hidden': 32, 'opt_class': <class 'torch.optim.adam.Adam'>, 'lr': 0.001}\n",
      "Performing training for: {'n_hidden': 32, 'opt_class': <class 'torch.optim.sgd.SGD'>, 'lr': 0.1}\n",
      "Performing training for: {'n_hidden': 32, 'opt_class': <class 'torch.optim.sgd.SGD'>, 'lr': 0.01}\n",
      "Performing training for: {'n_hidden': 64, 'opt_class': <class 'torch.optim.adam.Adam'>, 'lr': 0.001}\n",
      "Performing training for: {'n_hidden': 64, 'opt_class': <class 'torch.optim.sgd.SGD'>, 'lr': 0.1}\n",
      "Performing training for: {'n_hidden': 64, 'opt_class': <class 'torch.optim.sgd.SGD'>, 'lr': 0.01}\n"
     ]
    }
   ],
   "source": [
    "for param in params:\n",
    "    n_hidden = param[\"n_hidden\"]\n",
    "    opt_class = param[\"opt_class\"]\n",
    "    lr = param[\"lr\"]\n",
    "    \n",
    "    print(\"Performing training for: \" + str(param))\n",
    "    \n",
    "    pt=torch.load(\"pretrained_encoder.pt\", map_location='cuda:0')['model'][0].state_dict()\n",
    "    pretrained_model = CnnClassifier(n_hidden, pt_layers=pt).to(DEVICE)\n",
    "    optimizer = opt_class(pretrained_model.parameters(), lr=lr)\n",
    "    writer = SummaryWriter('./logs/cnn/%s' % opt_class.__name__)\n",
    "    \n",
    "    train(pretrained_model, dataloaders_validation, optimizer, writer, n_epochs, ckpt_path, DEVICE, \"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaeDecoder(nn.Module):\n",
    "     # n_hidden: number of units at the last fc layer\n",
    "    def __init__(self):\n",
    "        super(CaeDecoder, self).__init__()\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=32, out_channels=16, kernel_size=(3, 3), stride=(1, 1), padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=16, out_channels=8, kernel_size=(3, 3), stride=(1, 1), padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=8, out_channels=8, kernel_size=(3, 3), stride=(2, 2), padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=8, out_channels=4, kernel_size=(3, 3), stride=(1, 1), padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=4, out_channels=1, kernel_size=(4, 4), stride=(2, 2), padding=0),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.loss_fn = nn.MSELoss(reduction='sum')\n",
    "\n",
    "    def forward(self, in_data):\n",
    "        print(in_data.size())\n",
    "        img = self.conv_layers(in_data).view(in_data.size(0), 32)\n",
    "        return img\n",
    "\n",
    "    def loss(self, img, img_in):\n",
    "        return self.loss_fn(img, img_in) / img.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_decoder(model, loaders, optimizer, writer, n_epochs, ckpt_path, device):\n",
    "    pt=torch.load(\"pretrained_encoder.pt\", map_location='cuda:0')['model'][0]\n",
    "    \n",
    "    def run_epoch(train_or_eval):\n",
    "        epoch_loss = 0.\n",
    "        for i, batch in enumerate(loaders[train_or_eval], 1):\n",
    "            in_data, labels = batch\n",
    "            in_data, labels = in_data.to(device), labels.to(device)\n",
    "            \n",
    "            if train_or_eval == 'train':\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            img_features = pt(in_data)\n",
    "            imgs = model(img_features)\n",
    "            batch_loss = model.loss(imgs, in_data)\n",
    "\n",
    "            epoch_loss += batch_loss.item()\n",
    "\n",
    "            if train_or_eval == 'train':\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        epoch_loss /= i\n",
    "\n",
    "        losses[train_or_eval] = epoch_loss\n",
    "\n",
    "        if writer is None:\n",
    "            print('epoch %d %s loss %.4f ' % (epoch, train_or_eval, epoch_loss))\n",
    "        elif train_or_eval == 'eval':\n",
    "            name = \"lr-%.3f\" % (optimizer.param_groups[0][\"lr\"])\n",
    "            writer.add_scalars('%s_loss' % name,\n",
    "                               tag_scalar_dict={'train': losses['train'],\n",
    "                                                'eval': losses['eval']},\n",
    "                               global_step=epoch)\n",
    "\n",
    "            # For instructional purpose, add images here, just the last in_data\n",
    "            if epoch % 10 == 0:\n",
    "                if len(in_data.size()) == 2:  # when it is flattened, reshape it\n",
    "                    in_data = in_data.view(-1, 1, 28, 28)\n",
    "\n",
    "                img_grid = make_grid(in_data.to('cpu'))\n",
    "                writer.add_image('%s/eval_input' % model.__class__.__name__, img_grid, epoch)\n",
    "\n",
    "    # main statements\n",
    "    losses = dict()\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        run_epoch('train')\n",
    "        run_epoch('eval')\n",
    "\n",
    "        # For instructional purpose, show how to save checkpoints\n",
    "        if ckpt_path is not None:\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'losses': losses\n",
    "            }, '%s/%d.pt' % (ckpt_path, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_params = [{\"opt_class\": Adam, \"lr\": 0.001},\n",
    "                  {\"opt_class\": SGD, \"lr\": 0.1},\n",
    "                  {\"opt_class\": SGD, \"lr\": 0.01}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing training for: {'opt_class': <class 'torch.optim.adam.Adam'>, 'lr': 0.001}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (30) : unknown error at ..\\aten\\src\\THC\\THCGeneral.cpp:87",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-ffc4ecea65c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Performing training for: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCaeDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopt_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mwriter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./logs/cae/%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mopt_class\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mto\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    379\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 381\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    382\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_backward_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    191\u001b[0m                 \u001b[1;31m# Tensors stored in modules are graph leaves, and we don't\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m                 \u001b[1;31m# want to create copy nodes, so we have to unpack the data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m                 \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_grad\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m                     \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 379\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    160\u001b[0m             \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[0;32m    161\u001b[0m     \u001b[0m_check_driver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m     \u001b[0m_cudart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_load_cudart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[0m_cudart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudaGetErrorName\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_char_p\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: cuda runtime error (30) : unknown error at ..\\aten\\src\\THC\\THCGeneral.cpp:87"
     ]
    }
   ],
   "source": [
    "for param in decoder_params:\n",
    "    opt_class = param[\"opt_class\"]\n",
    "    lr = param[\"lr\"]\n",
    "    \n",
    "    print(\"Performing training for: \" + str(param))\n",
    "    \n",
    "    model = CaeDecoder().to(DEVICE)\n",
    "    optimizer = opt_class(model.parameters(), lr=lr)\n",
    "    writer = SummaryWriter('./logs/cae/%s' % opt_class.__name__)\n",
    "    \n",
    "    train_decoder(model, dataloaders_validation, optimizer, writer, n_epochs, ckpt_path, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
